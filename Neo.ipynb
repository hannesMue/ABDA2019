{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannesMue/ABDA2019/blob/master/Neo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76iSG5ju5Cd2",
        "colab_type": "text"
      },
      "source": [
        "**Import der Testdaten von GitHub**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZBlhDHvYnDt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d655e305-e92f-496f-bae1-d27de6c219bb"
      },
      "source": [
        "#!rm -rf SparkTestsData/\n",
        "#!ls SparkTestData"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'SparkTestData': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN6t94w6-QtK",
        "colab_type": "code",
        "outputId": "3b1f4f9b-8f55-4fdc-9006-eadb3d0f6a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/hannesMue/SparkTestsData.git"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SparkTestsData'...\n",
            "remote: Enumerating objects: 70, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/70)\u001b[K\rremote: Counting objects:   2% (2/70)\u001b[K\rremote: Counting objects:   4% (3/70)\u001b[K\rremote: Counting objects:   5% (4/70)\u001b[K\rremote: Counting objects:   7% (5/70)\u001b[K\rremote: Counting objects:   8% (6/70)\u001b[K\rremote: Counting objects:  10% (7/70)\u001b[K\rremote: Counting objects:  11% (8/70)\u001b[K\rremote: Counting objects:  12% (9/70)\u001b[K\rremote: Counting objects:  14% (10/70)\u001b[K\rremote: Counting objects:  15% (11/70)\u001b[K\rremote: Counting objects:  17% (12/70)\u001b[K\rremote: Counting objects:  18% (13/70)\u001b[K\rremote: Counting objects:  20% (14/70)\u001b[K\rremote: Counting objects:  21% (15/70)\u001b[K\rremote: Counting objects:  22% (16/70)\u001b[K\rremote: Counting objects:  24% (17/70)\u001b[K\rremote: Counting objects:  25% (18/70)\u001b[K\rremote: Counting objects:  27% (19/70)\u001b[K\rremote: Counting objects:  28% (20/70)\u001b[K\rremote: Counting objects:  30% (21/70)\u001b[K\rremote: Counting objects:  31% (22/70)\u001b[K\rremote: Counting objects:  32% (23/70)\u001b[K\rremote: Counting objects:  34% (24/70)\u001b[K\rremote: Counting objects:  35% (25/70)\u001b[K\rremote: Counting objects:  37% (26/70)\u001b[K\rremote: Counting objects:  38% (27/70)\u001b[K\rremote: Counting objects:  40% (28/70)\u001b[K\rremote: Counting objects:  41% (29/70)\u001b[K\rremote: Counting objects:  42% (30/70)\u001b[K\rremote: Counting objects:  44% (31/70)\u001b[K\rremote: Counting objects:  45% (32/70)\u001b[K\rremote: Counting objects:  47% (33/70)\u001b[K\rremote: Counting objects:  48% (34/70)\u001b[K\rremote: Counting objects:  50% (35/70)\u001b[K\rremote: Counting objects:  51% (36/70)\u001b[K\rremote: Counting objects:  52% (37/70)\u001b[K\rremote: Counting objects:  54% (38/70)\u001b[K\rremote: Counting objects:  55% (39/70)\u001b[K\rremote: Counting objects:  57% (40/70)\u001b[K\rremote: Counting objects:  58% (41/70)\u001b[K\rremote: Counting objects:  60% (42/70)\u001b[K\rremote: Counting objects:  61% (43/70)\u001b[K\rremote: Counting objects:  62% (44/70)\u001b[K\rremote: Counting objects:  64% (45/70)\u001b[K\rremote: Counting objects:  65% (46/70)\u001b[K\rremote: Counting objects:  67% (47/70)\u001b[K\rremote: Counting objects:  68% (48/70)\u001b[K\rremote: Counting objects:  70% (49/70)\u001b[K\rremote: Counting objects:  71% (50/70)\u001b[K\rremote: Counting objects:  72% (51/70)\u001b[K\rremote: Counting objects:  74% (52/70)\u001b[K\rremote: Counting objects:  75% (53/70)\u001b[K\rremote: Counting objects:  77% (54/70)\u001b[K\rremote: Counting objects:  78% (55/70)\u001b[K\rremote: Counting objects:  80% (56/70)\u001b[K\rremote: Counting objects:  81% (57/70)\u001b[K\rremote: Counting objects:  82% (58/70)\u001b[K\rremote: Counting objects:  84% (59/70)\u001b[K\rremote: Counting objects:  85% (60/70)\u001b[K\rremote: Counting objects:  87% (61/70)\u001b[K\rremote: Counting objects:  88% (62/70)\u001b[K\rremote: Counting objects:  90% (63/70)\u001b[K\rremote: Counting objects:  91% (64/70)\u001b[K\rremote: Counting objects:  92% (65/70)\u001b[K\rremote: Counting objects:  94% (66/70)\u001b[K\rremote: Counting objects:  95% (67/70)\u001b[K\rremote: Counting objects:  97% (68/70)\u001b[K\rremote: Counting objects:  98% (69/70)\u001b[K\rremote: Counting objects: 100% (70/70)\u001b[K\rremote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/64)\u001b[K\rremote: Compressing objects:   3% (2/64)\u001b[K\rremote: Compressing objects:   4% (3/64)\u001b[K\rremote: Compressing objects:   6% (4/64)\u001b[K\rremote: Compressing objects:   7% (5/64)\u001b[K\rremote: Compressing objects:   9% (6/64)\u001b[K\rremote: Compressing objects:  10% (7/64)\u001b[K\rremote: Compressing objects:  12% (8/64)\u001b[K\rremote: Compressing objects:  14% (9/64)\u001b[K\rremote: Compressing objects:  15% (10/64)\u001b[K\rremote: Compressing objects:  17% (11/64)\u001b[K\rremote: Compressing objects:  18% (12/64)\u001b[K\rremote: Compressing objects:  20% (13/64)\u001b[K\rremote: Compressing objects:  21% (14/64)\u001b[K\rremote: Compressing objects:  23% (15/64)\u001b[K\rremote: Compressing objects:  25% (16/64)\u001b[K\rremote: Compressing objects:  26% (17/64)\u001b[K\rremote: Compressing objects:  28% (18/64)\u001b[K\rremote: Compressing objects:  29% (19/64)\u001b[K\rremote: Compressing objects:  31% (20/64)\u001b[K\rremote: Compressing objects:  32% (21/64)\u001b[K\rremote: Compressing objects:  34% (22/64)\u001b[K\rremote: Compressing objects:  35% (23/64)\u001b[K\rremote: Compressing objects:  37% (24/64)\u001b[K\rremote: Compressing objects:  39% (25/64)\u001b[K\rremote: Compressing objects:  40% (26/64)\u001b[K\rremote: Compressing objects:  42% (27/64)\u001b[K\rremote: Compressing objects:  43% (28/64)\u001b[K\rremote: Compressing objects:  45% (29/64)\u001b[K\rremote: Compressing objects:  46% (30/64)\u001b[K\rremote: Compressing objects:  48% (31/64)\u001b[K\rremote: Compressing objects:  50% (32/64)\u001b[K\rremote: Compressing objects:  51% (33/64)\u001b[K\rremote: Compressing objects:  53% (34/64)\u001b[K\rremote: Compressing objects:  54% (35/64)\u001b[K\rremote: Compressing objects:  56% (36/64)\u001b[K\rremote: Compressing objects:  57% (37/64)\u001b[K\rremote: Compressing objects:  59% (38/64)\u001b[K\rremote: Compressing objects:  60% (39/64)\u001b[K\rremote: Compressing objects:  62% (40/64)\u001b[K\rremote: Compressing objects:  64% (41/64)\u001b[K\rremote: Compressing objects:  65% (42/64)\u001b[K\rremote: Compressing objects:  67% (43/64)\u001b[K\rremote: Compressing objects:  68% (44/64)\u001b[K\rremote: Compressing objects:  70% (45/64)\u001b[K\rremote: Compressing objects:  71% (46/64)\u001b[K\rremote: Compressing objects:  73% (47/64)\u001b[K\rremote: Compressing objects:  75% (48/64)\u001b[K\rremote: Compressing objects:  76% (49/64)\u001b[K\rremote: Compressing objects:  78% (50/64)\u001b[K\rremote: Compressing objects:  79% (51/64)\u001b[K\rremote: Compressing objects:  81% (52/64)\u001b[K\rremote: Compressing objects:  82% (53/64)\u001b[K\rremote: Compressing objects:  84% (54/64)\u001b[K\rremote: Compressing objects:  85% (55/64)\u001b[K\rremote: Compressing objects:  87% (56/64)\u001b[K\rremote: Compressing objects:  89% (57/64)\u001b[K\rremote: Compressing objects:  90% (58/64)\u001b[K\rremote: Compressing objects:  92% (59/64)\u001b[K\rremote: Compressing objects:  93% (60/64)\u001b[K\rremote: Compressing objects:  95% (61/64)\u001b[K\rremote: Compressing objects:  96% (62/64)\u001b[K\rremote: Compressing objects:  98% (63/64)\u001b[K\rremote: Compressing objects: 100% (64/64)\u001b[K\rremote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "Unpacking objects:   1% (1/70)   \rUnpacking objects:   2% (2/70)   \rUnpacking objects:   4% (3/70)   \rUnpacking objects:   5% (4/70)   \rUnpacking objects:   7% (5/70)   \rUnpacking objects:   8% (6/70)   \rUnpacking objects:  10% (7/70)   \rUnpacking objects:  11% (8/70)   \rUnpacking objects:  12% (9/70)   \rUnpacking objects:  14% (10/70)   \rUnpacking objects:  15% (11/70)   \rUnpacking objects:  17% (12/70)   \rUnpacking objects:  18% (13/70)   \rUnpacking objects:  20% (14/70)   \rremote: Total 70 (delta 19), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  21% (15/70)   \rUnpacking objects:  22% (16/70)   \rUnpacking objects:  24% (17/70)   \rUnpacking objects:  25% (18/70)   \rUnpacking objects:  27% (19/70)   \rUnpacking objects:  28% (20/70)   \rUnpacking objects:  30% (21/70)   \rUnpacking objects:  31% (22/70)   \rUnpacking objects:  32% (23/70)   \rUnpacking objects:  34% (24/70)   \rUnpacking objects:  35% (25/70)   \rUnpacking objects:  37% (26/70)   \rUnpacking objects:  38% (27/70)   \rUnpacking objects:  40% (28/70)   \rUnpacking objects:  41% (29/70)   \rUnpacking objects:  42% (30/70)   \rUnpacking objects:  44% (31/70)   \rUnpacking objects:  45% (32/70)   \rUnpacking objects:  47% (33/70)   \rUnpacking objects:  48% (34/70)   \rUnpacking objects:  50% (35/70)   \rUnpacking objects:  51% (36/70)   \rUnpacking objects:  52% (37/70)   \rUnpacking objects:  54% (38/70)   \rUnpacking objects:  55% (39/70)   \rUnpacking objects:  57% (40/70)   \rUnpacking objects:  58% (41/70)   \rUnpacking objects:  60% (42/70)   \rUnpacking objects:  61% (43/70)   \rUnpacking objects:  62% (44/70)   \rUnpacking objects:  64% (45/70)   \rUnpacking objects:  65% (46/70)   \rUnpacking objects:  67% (47/70)   \rUnpacking objects:  68% (48/70)   \rUnpacking objects:  70% (49/70)   \rUnpacking objects:  71% (50/70)   \rUnpacking objects:  72% (51/70)   \rUnpacking objects:  74% (52/70)   \rUnpacking objects:  75% (53/70)   \rUnpacking objects:  77% (54/70)   \rUnpacking objects:  78% (55/70)   \rUnpacking objects:  80% (56/70)   \rUnpacking objects:  81% (57/70)   \rUnpacking objects:  82% (58/70)   \rUnpacking objects:  84% (59/70)   \rUnpacking objects:  85% (60/70)   \rUnpacking objects:  87% (61/70)   \rUnpacking objects:  88% (62/70)   \rUnpacking objects:  90% (63/70)   \rUnpacking objects:  91% (64/70)   \rUnpacking objects:  92% (65/70)   \rUnpacking objects:  94% (66/70)   \rUnpacking objects:  95% (67/70)   \rUnpacking objects:  97% (68/70)   \rUnpacking objects:  98% (69/70)   \rUnpacking objects: 100% (70/70)   \rUnpacking objects: 100% (70/70), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dfu1-D_-doD",
        "colab_type": "code",
        "outputId": "3f448dbc-ecb0-4086-cc55-a919cd6c2e0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls SparkTestsData/Testdaten"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eth_eur.CSV  ltc_usd.CSV  test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hwCK6sV5Mxn",
        "colab_type": "text"
      },
      "source": [
        "**Installation Spark**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M04vNVvE-6KD",
        "colab_type": "code",
        "outputId": "5249a70a-382a-4c28-ce9a-09e06b63292e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install findspark"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.6/dist-packages (1.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cyzq55I-9UQ",
        "colab_type": "code",
        "outputId": "22bcd312-024e-4203-ad0d-568ca75c91fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!ls spark-2.4.4-bin-hadoop2.7\n",
        "!apt-get install tree"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tree is already the newest version (1.7.0-5).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq2jryu-6GtW",
        "colab_type": "text"
      },
      "source": [
        "**Installation von was!?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV1ZZZ5i_Cr0",
        "colab_type": "code",
        "outputId": "046d8ea9-ab5b-47ef-a8a0-2d39f8dabb32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install ts ts-flint"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ts in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: ts-flint in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_SQhDW_FCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "#spark = SparkSession.builder.config('spark.driver.extraClassPath', '/path/to/postgresql.jar').getOrCreate()\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q95gzcZ681H",
        "colab_type": "text"
      },
      "source": [
        "**Testen von Spark Umgebung**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFlKziW3_Wta",
        "colab_type": "code",
        "outputId": "d0db51e6-5c1b-4782-be60-74b3ff714c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3)\n",
        "df.count()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV7KZ7w5_kZR",
        "colab_type": "text"
      },
      "source": [
        "**Überprüfen der Daten**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l301MEiH_tey",
        "colab_type": "code",
        "outputId": "b4014c75-894c-4f20-fb76-7f289843037a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "df_spark_eth = spark.read.csv('./SparkTestsData/Testdaten/eth_eur.CSV', inferSchema=True, header=True)\n",
        "from pyspark.sql.functions import input_file_name\n",
        "df_spark_eth = df_spark_eth.withColumn(\"filename\",input_file_name())\n",
        "\n",
        "print(type(df_spark_eth))\n",
        "df_spark_eth.printSchema()\n",
        "df_spark_eth.show()\n",
        "df_spark_eth.count()\n",
        "\n",
        "!head -n 2 ./SparkTestsData/Testdaten/eth_eur.csv"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "root\n",
            " |-- Datum: string (nullable = true)\n",
            " |-- Timestamp: integer (nullable = true)\n",
            " |-- Closing: integer (nullable = true)\n",
            " |-- filename: string (nullable = false)\n",
            "\n",
            "+----------+----------+-------+--------------------+\n",
            "|     Datum| Timestamp|Closing|            filename|\n",
            "+----------+----------+-------+--------------------+\n",
            "|01.11.2019|1572566400|    165|file:///content/S...|\n",
            "|02.11.2019|1572652800|    164|file:///content/S...|\n",
            "|03.11.2019|1572739200|    165|file:///content/S...|\n",
            "|04.11.2019|1572825600|    162|file:///content/S...|\n",
            "|05.11.2019|1572912000|    168|file:///content/S...|\n",
            "|06.11.2019|1572998400|    175|file:///content/S...|\n",
            "|07.11.2019|1573084800|    170|file:///content/S...|\n",
            "|08.11.2019|1573171200|    169|file:///content/S...|\n",
            "|09.11.2019|1573257600|    167|file:///content/S...|\n",
            "|10.11.2019|1573344000|    171|file:///content/S...|\n",
            "|11.11.2019|1573430400|    171|file:///content/S...|\n",
            "|12.11.2019|1573516800|    168|file:///content/S...|\n",
            "|13.11.2019|1573603200|    169|file:///content/S...|\n",
            "|14.11.2019|1573689600|    169|file:///content/S...|\n",
            "|15.11.2019|1573776000|    165|file:///content/S...|\n",
            "|16.11.2019|1573862400|    164|file:///content/S...|\n",
            "|17.11.2019|1573948800|    165|file:///content/S...|\n",
            "|18.11.2019|1574035200|    166|file:///content/S...|\n",
            "|19.11.2019|1574121600|    160|file:///content/S...|\n",
            "|20.11.2019|1574208000|    159|file:///content/S...|\n",
            "+----------+----------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "head: cannot open './SparkTestsData/Testdaten/eth_eur.csv' for reading: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNY5H_twisAS",
        "colab_type": "code",
        "outputId": "b62b3671-43fd-4946-b985-71679c542d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "df_spark_eth.createOrReplaceTempView(\"eth\")\n",
        "#Einige Versuche:\n",
        "\n",
        "#df_spark_eth.select(to_date(eth.Datum).alias('Datum')).collect()\n",
        "\n",
        "#basis_daten = spark.sql(\n",
        "#    \"SELECT Datum,CAST(Datum As Date), Closing, AS formatted_date FROM eth\")\n",
        "\n",
        "#Lösung: CSV noch zusätzlich mit Timestamp aufgefüllt\n",
        "\n",
        "#Spalte für Closing vom Vortag erzeugen um \n",
        "basis_daten = spark.sql(\n",
        "    \"SELECT Datum,Timestamp,CAST(Timestamp AS Timestamp) AS formatted_date, Closing FROM eth\" \n",
        "    #WHERE a.formatted_date ORDER BY formatted_date DESC \"\n",
        ")\n",
        "\n",
        "basis_daten.show()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+-------------------+-------+\n",
            "|     Datum| Timestamp|     formatted_date|Closing|\n",
            "+----------+----------+-------------------+-------+\n",
            "|01.11.2019|1572566400|2019-11-01 00:00:00|    165|\n",
            "|02.11.2019|1572652800|2019-11-02 00:00:00|    164|\n",
            "|03.11.2019|1572739200|2019-11-03 00:00:00|    165|\n",
            "|04.11.2019|1572825600|2019-11-04 00:00:00|    162|\n",
            "|05.11.2019|1572912000|2019-11-05 00:00:00|    168|\n",
            "|06.11.2019|1572998400|2019-11-06 00:00:00|    175|\n",
            "|07.11.2019|1573084800|2019-11-07 00:00:00|    170|\n",
            "|08.11.2019|1573171200|2019-11-08 00:00:00|    169|\n",
            "|09.11.2019|1573257600|2019-11-09 00:00:00|    167|\n",
            "|10.11.2019|1573344000|2019-11-10 00:00:00|    171|\n",
            "|11.11.2019|1573430400|2019-11-11 00:00:00|    171|\n",
            "|12.11.2019|1573516800|2019-11-12 00:00:00|    168|\n",
            "|13.11.2019|1573603200|2019-11-13 00:00:00|    169|\n",
            "|14.11.2019|1573689600|2019-11-14 00:00:00|    169|\n",
            "|15.11.2019|1573776000|2019-11-15 00:00:00|    165|\n",
            "|16.11.2019|1573862400|2019-11-16 00:00:00|    164|\n",
            "|17.11.2019|1573948800|2019-11-17 00:00:00|    165|\n",
            "|18.11.2019|1574035200|2019-11-18 00:00:00|    166|\n",
            "|19.11.2019|1574121600|2019-11-19 00:00:00|    160|\n",
            "|20.11.2019|1574208000|2019-11-20 00:00:00|    159|\n",
            "+----------+----------+-------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}